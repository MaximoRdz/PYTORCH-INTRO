{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697ded1e",
   "metadata": {},
   "source": [
    "# Lesson 4: Cross-Entropy Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2652597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "plt.style.use(['science', 'notebook'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d332738",
   "metadata": {},
   "source": [
    "## An initial problem\n",
    "\n",
    "Suppose I have a set of numbers $\\vec{p} = [1,3,5,2,...]$ of length $N$. I want to know what the corresponding matching set of numbers $\\vec{q} = [q_1,q_2,q_3,...]$ is such that the following function is minimized: \n",
    "\n",
    "$$H(\\vec{p}, \\vec{q}) = - \\sum^{N}_{i=1} p_i ln (q_i)$$\n",
    "\n",
    "with the constraint that $\\sum_{i} p_i = \\sum_{i} q_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "687c441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([5,1,4,6,2,4])\n",
    "q1 = p\n",
    "q2 = np.array([3,7,1,4,1,6])\n",
    "q3 = np.array([2,5,7,2,1,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccd9b8f",
   "metadata": {},
   "source": [
    "Obviously the defined vectors meet the constraint mentioned before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "162dd5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22 22 22\n"
     ]
    }
   ],
   "source": [
    "print(sum(p), sum(q1), sum(q2), sum(q3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dfa938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sum_Log(p, q):\n",
    "    return - sum(p*np.log(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7c0a5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-31.27439562761785, -22.923775636027425, -23.455449144551153]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Sum_Log(p, i) for i in [q1, q2, q3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80cd07f",
   "metadata": {},
   "source": [
    "It turns out that the value that minimizes $H(\\vec{p}, \\vec{q})$ is $\\vec{q} = \\vec{p}$ (proved via Lagrange Multipliers)\n",
    "\n",
    "### Cross Entropy Loss\n",
    "\n",
    "In classification problems, an input is taken in, such as an image. We'll call this x. This image may, for example, contain 1 of 5 different objects (dog, cat, house, ...). We'll call the **true likelihood** of an image x belonging to class $i$ as $p_i$ (length $5$ vector) The goal of a classifier is to create a function $f$ such that\n",
    "\n",
    "$$f(x) = \\vec{q}$$\n",
    "\n",
    "where $\\vec{q}$ (length $5$ vector) is as close to $\\vec{p}$ as possible.\n",
    "\n",
    "- Note that $\\vec{p}$ and $\\vec{q}$ are probability mass functions, and each element of the vector represents a different class. It follows that $\\sum_{i} p_i = \\sum_{i} q_i = 1$\n",
    "- Typically we know what class $\\hat{c}$ an image $x$ belongs to. In this case, its typically the case that $p_{\\hat{c}} = 1$ for the class $i = \\hat{c}$ that we know it is, and all the other components are equal to zero.\n",
    "\n",
    "To minimize the difference between $\\vec{p}$ and $\\vec{q}$, we can minimize the **cross entropy loss function**\n",
    "\n",
    "$$H(\\vec{p}, \\vec{q}) = - \\sum^{N}_{i=1} p_i ln (q_i)$$\n",
    "\n",
    "Since the minimum of this function occurs precisely when $q_i = p_i$ for all $i$. In the case where we know which class an image belongs to (i.e. one of the $p_i$ is $1$ and the rest are $0$) we have:\n",
    "\n",
    "$$H(\\vec{p}, \\vec{q}) = - ln (q_\\hat{c})$$\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18df7522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p is the pmf with 10 categories and in this case the image belongs to the fifth since p[4] = 1\n",
    "p = np.zeros(10, dtype = int); p[4] = 1\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87145890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05832266, 0.00710061, 0.09202471, 0.17511918, 0.06094468,\n",
       "       0.19554355, 0.04165075, 0.05929879, 0.2142208 , 0.09577427])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q is the random pmf our 'model' predicts initially\n",
    "q = np.random.rand(10); q /= sum(q) # normalized\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2800840b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.79778871])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross entropy loss for that prediction\n",
    "H = -np.log(q[p>0])\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75859d93",
   "metadata": {},
   "source": [
    "What if we make the probability higher of this fifth category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95421c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.38956060e-04, 1.69174805e-05, 2.19252545e-04, 4.17228425e-04,\n",
       "       9.97762663e-01, 4.65890284e-04, 9.92345696e-05, 1.41281738e-04,\n",
       "       5.10389584e-04, 2.28186009e-04])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[4] = 20; q /= sum(q) # renormalization of q\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aead8b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00223984])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = -np.log(q[p>0])\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e86eff",
   "metadata": {},
   "source": [
    "now there is a higher probability that the fifth class is the correct prediction, this makes sense since the cross entropy measures the similarity of the two so as it is getting smaller the similarity is increasing\n",
    "\n",
    "### Multiple images case:\n",
    "\n",
    "In this case, consider us computing this loss over N images (multiple images $x_n$). Suppose also that we know exactly what class the image belongs to. We can thus express the true class of the nth image as $\\hat{c}(n)$. We will express the probability of image $n$ belonging to class $c$ as $q_n(c)$.\n",
    "\n",
    "- Thus the predicted probability of the image $x_n$ belonging to its true class $\\hat{c}(n)$ is $q_n(\\hat{c}(n))$\n",
    "\n",
    "We sum it together: \n",
    "\n",
    "$$L(p, q) = \\sum^{N}_{n=1} H(p_n, q_n) = - \\sum^{N}_{n=1} ln (q(\\hat{c}(n))$$\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c32ee366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pmf of each image (row-wise) each image can belong to ten different classes\n",
    "p = np.zeros((4, 10), dtype = int)\n",
    "p[0, 4] = 1\n",
    "p[1, 2] = 1\n",
    "p[2, 8] = 1\n",
    "p[3, 6] = 1\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40e209a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18186801, 0.05261113, 0.14999476, 0.07704911, 0.1938232 ,\n",
       "        0.02940752, 0.00988255, 0.09391811, 0.04980343, 0.16164218],\n",
       "       [0.03718438, 0.14422244, 0.19718861, 0.03476578, 0.10705033,\n",
       "        0.1433361 , 0.03161328, 0.1358876 , 0.16380204, 0.00494945],\n",
       "       [0.15827281, 0.12243628, 0.09479612, 0.12261051, 0.06296463,\n",
       "        0.08616485, 0.1297719 , 0.02392835, 0.06144067, 0.13761388],\n",
       "       [0.13533321, 0.10988344, 0.00276084, 0.11270723, 0.18003496,\n",
       "        0.11734929, 0.12199301, 0.00741246, 0.03662287, 0.1759027 ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = np.random.rand(40).reshape(4,10)\n",
    "# normalizing each image total prob\n",
    "for i in range(4):\n",
    "    q[i, :] /= sum(q[i, :])\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "115e4a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01580981, 0.14170686, 0.00465834, 0.13535099, 0.1361967 ,\n",
       "        0.12893708, 0.10660179, 0.1026213 , 0.10377294, 0.12434419],\n",
       "       [0.13202032, 0.06455847, 0.13208139, 0.16878477, 0.16132915,\n",
       "        0.00127524, 0.18918898, 0.04025855, 0.08213276, 0.02837036],\n",
       "       [0.07193565, 0.16561049, 0.07452134, 0.08693843, 0.014086  ,\n",
       "        0.12766727, 0.13311377, 0.05452521, 0.16718668, 0.10441517],\n",
       "       [0.12610882, 0.04518115, 0.01601851, 0.14483608, 0.18883643,\n",
       "        0.12819185, 0.06250566, 0.03809882, 0.11044634, 0.13977633]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizing q USING BROADCASTING RULES \n",
    "q = np.random.rand(40).reshape(4,10)\n",
    "q /= np.expand_dims(np.sum(q, axis = 1), axis = 1)\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e2dc2e",
   "metadata": {},
   "source": [
    "Compute $H$ for each term in the sum and then sum together to get $L$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5669cec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.579134447144245"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hs = -np.log(q[p>0])\n",
    "L = sum(Hs)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5791f8",
   "metadata": {},
   "source": [
    "## How do we get the q's in Machine Learning?\n",
    "\n",
    "$\\vec{q}$ should be related to a probability density function\n",
    "\n",
    "- Bounded between $0$ and $1$\n",
    "- The closer to $0$, the less likely we are confident that image $n$ is class $c$\n",
    "- The closer to $1$, the more likely we are confident that image $n$ is class $c$\n",
    "- $\\sum^{C}_{c} q_c = 1$ for each image (we enforce this in the previous examples in a different way than PyTorch)\n",
    "\n",
    "Suppose a Neural Network outputs $f(x_n) = \\hat{y}_n$ where $\\hat{y}_n$ is a vector with the same length as the number of classes, but $\\hat{y}_n$ is not normalized like $\\vec{q}$ should be. We can enforce the last condition by normalizing the following way (PyTorch's way)\n",
    "\n",
    "$$q_n(c) = \\frac{ exp(\\hat{y}_n (c)) }{ \\sum^{C}_{c'=0}exp(\\hat{y}_n (c')) }$$\n",
    "\n",
    "So we can rewrite our loss function as\n",
    "\n",
    "$$L(\\hat{y}) = - \\sum^{N}_{n=0} ln (q_n(\\tilde{c}(n)) = - \\sum^{N}_{n=0} \\frac{ exp(\\hat{y}_n (\\tilde{c})) }{ \\sum^{C}_{c=0}exp(\\hat{y}_n (c)) }$$\n",
    "\n",
    "Get sample $\\hat{y}$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a65b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.57614589e-01, 5.59978652e+00, 1.80661801e+01, 1.32723878e+01,\n",
       "        1.17101353e+01, 1.93854974e+00, 1.40678832e+01, 1.52104690e+01,\n",
       "        5.99066838e+00, 1.82897163e+01],\n",
       "       [1.22847764e+00, 8.14917532e-01, 4.99672157e-01, 6.51800729e-01,\n",
       "        7.53291377e+00, 1.90712688e+01, 1.29011070e-02, 5.56937232e+00,\n",
       "        2.20279184e+00, 2.35152472e+00],\n",
       "       [2.43325579e-01, 1.23062427e+01, 9.09630917e-01, 9.45112296e+00,\n",
       "        1.95408592e+01, 1.65091263e+01, 1.01041873e+01, 1.94139224e+00,\n",
       "        9.84814328e+00, 1.94443891e-01],\n",
       "       [1.92347521e+01, 4.19035110e+00, 2.31099677e-01, 8.41312546e+00,\n",
       "        7.11109410e+00, 1.31115778e+00, 1.83897563e+00, 2.09734709e+00,\n",
       "        5.78187038e+00, 1.11305076e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows correspond to a specific image, columns to each possible class the image belongs to\n",
    "# as usual with NN is not normalized\n",
    "yhat = 20*np.random.rand(40).reshape(4, 10)**2\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d6b229d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = np.exp(yhat)\n",
    "# normalizating pytorch like\n",
    "q /= np.expand_dims(np.sum(q, axis = 1), axis = 1)\n",
    "np.sum(q, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c95b15",
   "metadata": {},
   "source": [
    "Compute $\\tilde{c}(n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6cc3f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pmf of each image (row-wise) each image can belong to ten different classes\n",
    "p = np.zeros((4, 10), dtype = int)\n",
    "p[0, 4] = 1\n",
    "p[1, 2] = 1\n",
    "p[2, 8] = 1\n",
    "p[3, 6] = 1\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5112746f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False,  True, False, False, False, False,\n",
       "        False],\n",
       "       [False, False,  True, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [False, False, False, False, False, False, False, False,  True,\n",
       "        False],\n",
       "       [False, False, False, False, False, False,  True, False, False,\n",
       "        False]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# real class index of each image (known beforehand)\n",
    "c_tilde = p>0\n",
    "c_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5d95f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.91274203723633"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hs = -np.log(q[c_tilde])\n",
    "L = np.sum(Hs)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71dc3ea",
   "metadata": {},
   "source": [
    "## Proof this is equal to PyTorch\n",
    "\n",
    "Create the Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "263ce6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = torch.nn.CrossEntropyLoss(reduction = 'sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3d0ca",
   "metadata": {},
   "source": [
    "Evaluate on data above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28ad877b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(52.9127, dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(torch.tensor(yhat), torch.tensor(p, dtype = torch.float))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
